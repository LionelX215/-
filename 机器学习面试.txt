1.请问GBDT和XGBoost的区别是什么？
	XGBoost类似于GBDT的优化版，不论是精度还是效率上都有了提升。与GBDT相比，具体的优点有： 
	(1.损失函数是用泰勒展式二项逼近，而不是像GBDT里的就是一阶导数； 
	(2.对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性； 
	(3.节点分裂的方式不同，GBDT是用的基尼系数，XGBoost是经过优化推导后的。

2.LR可不可以用平方误差做损失函数？与交叉熵相比有什么缺点或优点？ 
	可以，但是如果逻辑回归使用平方误差，那么损失函数是非凸的，在h(wx)接近0和1的地方梯度很小，
	不容易学习，而使用交叉熵作为损失函数，是收敛的，方便计算。
	
3.交叉熵的意义
	交叉熵损失函数可以衡量真实与预测的相似性，交叉熵作为损失函数还有一个好处是使用sigmoid函
	数在梯度下降时能避免均方误差损失函数学习速率降低的问题，因为学习速率可以被输出的误差所
	控制。把p(i)看作是真实的概率分布，q(i)看作是预测的概率分布，如果把交叉熵作为loss函数，当
	我们最小化它时，可以使q(i)逐渐逼近p(i)，也就达到了拟合的目的。
	
4.LR是如何求解的？
	LR模型的数学形式确定后，剩下就是如何去求解模型中的参数。统计学中常用的一种方法是最大似然
	估计（MLE），即找到一组参数，使得在这组参数下，我们的数据的似然度（概率）越大。对于该优化
	问题，存在多种求解方法，这里以梯度下降的为例说明。梯度下降(Gradient Descent)又叫作最速梯
	度下降，是一种迭代求解的方法，通过在每一步选取使目标函数变化最快的一个方向调整参数的值来逼近最优值。

5.L1正则化和L2正则化的区别。
	L2正则化项使值最小时对应的参数变小。L1正则化容易使参数为0，即特征稀疏化。

6.LR和SVM的异同
	相同点:
		1. 都是分类算法
		2. 都是监督学习算法
		3. 都是判别模型
		4. 都能通过核函数方法针对非线性情况分类
		5. 目标都是找一个分类超平面
		6. 都能减少离群点的影响
	不同点:
		1. 损失函数不同，逻辑回归是cross entropy loss，svm是hinge loss
		2. 逻辑回归在优化参数时所有样本点都参与了贡献，svm则只取离分离超平面最近的支持向量样本。这也是为什么逻辑回归不用核函数，它需要计算的样本太多。
		   并且由于逻辑回归受所有样本的影响，当样本不均衡时需要平衡一下每一类的样本个数。
		3. 逻辑回归对概率建模，svm对分类超平面建模
		4. 逻辑回归是处理经验风险最小化，svm是结构风险最小化。这点体现在svm自带L2正则化项，逻辑回归并没有
		5. 逻辑回归通过非线性变换减弱分离平面较远的点的影响，svm则只取支持向量从而消去较远点的影响
		6. 逻辑回归是统计方法，svm是几何方法

7.常用损失函数
	常见的损失误差有五种： 
		1. 铰链损失（Hinge Loss）：主要用于支持向量机（SVM） 中； 
		2. 互熵损失 （Cross Entropy Loss，Softmax Loss ）：用于Logistic 回归与Softmax 分类中； 
		3. 平方损失（Square Loss）：主要是最小二乘法（OLS）中； 
		4. 指数损失（Exponential Loss） ：主要用于Adaboost 集成学习算法中； 
		5. 其他损失（如0-1损失，绝对值损失）

8.LR是否可以引入核函数？
	可以。但是SVM中，影响决策面的数据只有支持向量，而在LR中，所有数据都会都决策面造成影响，所以在LR中引入
	核函数会带来很高的计算复杂度，因此一般在LR中不使用核函数。

9.SVM原理
	SVM是一种分类算法，目标是确定一个间隔最大化的分类超平面，将不同数据分隔开。
		(1).当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；
		(2).当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；
		(3).当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。
		硬间隔最大化（几何间隔）---学习的对偶问题---软间隔最大化（引入松弛变量）---非线性支持向量机（核技巧）。
	回顾 C=1/λ，因此：
	C 较大时，相当于 λ 较小，可能会导致过拟合，高方差。
	C 较小时，相当于 λ 较大，可能会导致低拟合，高偏差。
 
10.核函数有哪些？
	我们把这里的计算两个向量在隐式映射过后的空间中的内积的函数叫做核函数 (Kernel Function)
	核函数包括线性核函数、多项式核函数、高斯核函数等，其中高斯核函数最常用，可以将数据映射到无穷维，也叫做
	径向基函数（Radial Basis Function 简称 RBF），是某种沿径向对称的标量函数。 通常定义为空间中任一点x到某
	一中心xc之间欧氏距离的单调函数 , 可记作 k（||x-xc||）， 其作用往往是局部的 , 即当x远离xc时函数取值很小。
	一个是映射到高维空间中，然后再根据内积的公式进行计算。
	而另一个则直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果。

11.决策树
	决策树思想：表示给定特征条件下的条件概率分布，分类即将该节点分到条件概率大的一边，决策树是一个递归选择最优特
	征的过程，特征将训练机划分成子集，在当前条件下该状态是最好的分类  1）若子集能被正确分类，则构造叶节点 2）若自
	己没有完全被正确分类，则对自己选择新的最优特征（递归过程）终止条件：直到所有训练子集能被正确分类

12.过拟合
	当训练样例的错误率明显比测试样例的错误率低的时候，那么就说算法过度拟合训练数据，称为过拟合。
	出现过拟合的原因:
		(1).训练集的数量级和模型的复杂度不匹配。训练集的数量级要小于模型的复杂度；
		(2).训练集和测试集特征分布与训练集不一致；
		(4).权值学习迭代次数足够多(Overtraining)，拟合了训练数据中的噪声和训练样例中没有代表性的特征。
	解决方法：
		(1).调小模型复杂度，使其适合自己训练集的数量级（缩小宽度和减小深度）
		(2).训练集越多，过拟合的概率越小。在计算机视觉领域中，增广的方式是对图像旋转，缩放，剪切，添加噪声等。
		(3).参数太多，会导致我们的模型复杂度上升，容易过拟合，也就是我们的训练误差会很小。 正则化是指通过引入
			额外新信息来解决机器学习中过拟合问题的一种方法。这种额外信息通常的形式是模型复杂性带来的惩罚度。 正则
			化可以保持模型简单，另外，规则项的使用还可以约束我们的模型的特性。

13.为什么L1有特征选择的功能？ 
	大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，xi的大部分元素（也就是特征）都是
	和最终的输出yi没有关系或者不提供任何信息的，在最小化目标函数的时候考虑xi这些额外的特征，虽然可以获得更小的训
	练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确yi的预测。稀疏规则化算子的引入就是为
	了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。 

14.Batch Normalization的作用是什么
	对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较
	标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。因为梯度一直都能保
	持比较大的状态，所以很明显对神经网络的参数调整效率比较高，就是变动大，就是说向损失函数最优值迈动的步子大，也
	就是说收敛地快。
	前一层经过激活函数的输出，减去m个实例获得的m个激活x求得的均值并除以求得的方差来进行转换。

15.对比梯度下降、随机梯度下降和批量梯度下降
	梯度下降：梯度下降就是我上面的推导，要留意，在梯度下降中，对于θ的更新，所有的样本都有贡献，也就是参与调整θ.其
	计算得到的是一个标准梯度。因而理论上来说一次更新的幅度是比较大的。如果样本不多的情况下，当然是这样收敛的速度
	会更快
	随机梯度下降：可以看到多了随机两个字，随机也就是说我用样本中的一个例子来近似我所有的样本，来调整θ，因而随机梯度下
	降是会带来一定的问题，因为计算得到的并不是准确的一个梯度，容易陷入到局部最优解中
	批量梯度下降：其实批量的梯度下降就是一种折中的方法，他用了一些小样本来近似全部的，其本质就是我1个指不定不太准，那
	我用个30个50个样本那比随机的要准不少了吧，而且批量的话还是非常可以反映样本的一个分布情况的。

16.信息增益、熵和基尼值
	(1) 度量随机变量的不确定性。（纯度）。熵越大，混乱程度越高，也就是纯度越低；反之，熵越小，混乱程度越低，纯度越高。
	如果S的所有成员属于同一类，则Entropy(S)=0；如果S的正反样例数量相等，则Entropy(S)=1；如果S的正反样例数量不等，则熵介于0，1之间
	
	(2) 信息增益(ID3)：对于待划分的数据集D，其 entroy(前)是一定的，但是划分之后的熵 entroy(后)是不定的，entroy(后)越小说明使用此特征划分
	得到的子集的不确定性越小（也就是纯度越高），因此 entroy(前) -  entroy(后)差异越大，说明使用当前特征划分数据集D的话，其纯度上升的
	更快。而我们在构建最优的决策树的时候总希望能更快速到达纯度更高的集合，这一点可以参考优化算法中的梯度下降算法，每一步沿着负梯度方
	法最小化损失函数的原因就是负梯度方向是函数值减小最快的方向。同理：在决策树构建的过程中我们总是希望集合往最快到达纯度更高的子集合
	方向发展，因此我们总是选择使得信息增益最大的特征来划分当前数据集D。
	
	(3) 基尼指数(CART)：表示在样本集合中一个随机选中的样本被分错的概率,Gini指数越小表示集合中被选中的样本被分错的概率越小，也就是说集合
	的纯度越高，反之，集合越不纯。即 基尼指数（基尼不纯度）= 样本被选中的概率 * 样本被分错的概率。CART是二叉树。
	其同样以上述熵的二分类例子为例，当两类数量相等时，基尼值等于0.5；当节点数据属于同一类时，基尼值等于0。基尼值越大，数据越不纯。
	
	(4) 使用信息增比(C4.5)作为选择分裂的条件有一个不可避免的缺点：倾向选择分支比较多的属性进行分裂。为了解决这个问题，引
	入了信息增益率这个概念。信息增益是在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。
    惩罚参数：数据集D以特征A作为随机变量的熵的倒数，即：将特征A取值相同的样本划分到同一个子集中（之前所说数据集的熵是依据类别进行划分的）

17.停止分裂的条件
	（1）最小节点数
　　当节点的数据量小于一个指定的数量时，不继续分裂。两个原因：一是数据量较少时，再做分裂容易强化噪声数据的作用；二是降
	低树生长的复杂性。提前结束分裂一定程度上有利于降低过拟合的影响。
　　（2）熵或者基尼值小于阀值。
	由上述可知，熵和基尼值的大小表示数据的复杂程度，当熵或者基尼值过小时，表示数据的纯度比较大，如果熵或者基尼值小于一
	定程度数，节点停止分裂。
　　（3）决策树的深度达到指定的条件
　  节点的深度可以理解为节点与决策树跟节点的距离，如根节点的子节点的深度为1，因为这些节点与跟节点的距离为1，子节点的深
	度要比父节点的深度大1。决策树的深度是所有叶子节点的最大深度，当深度到达指定的上限大小时，停止分裂。
　　（4）所有特征已经使用完毕，不能继续进行分裂。
    被动式停止分裂的条件，当已经没有可分的属性时，直接将当前节点设置为叶子节点。

18.CART
	分类树是用基尼值做判断，选择基尼值小的作为分裂的方案。
	回归树是用方差做判断，选择方差小的作为分裂的方案。

19.判别式模型和生成式模型

	判别式模型（Discriminative Model）是直接对条件概率p(y|x;θ)建模。
	常见的判别式模型有：
	Logistic regression（logistical 回归）
	Linear discriminant analysis（线性判别分析）
	Supportvector machines（支持向量机）
	Boosting（集成学习）
	Conditional random fields（条件随机场）
	Linear regression（线性回归）
	Neural networks（神经网络）
	
　　生成式模型（Generative Model）则会对x和y的联合分布p(x,y)建模，然后通过贝叶斯公式来求得p(yi|x)，然后选取使得p(yi|x)最大的yi。
	常见的生成式模型有：
	Gaussian mixture model and othertypes of mixture model（高斯混合及其他类型混合模型）
	Hidden Markov model（隐马尔可夫）
	NaiveBayes（朴素贝叶斯）
	AODE（平均单依赖估计）
	Latent Dirichlet allocation（LDA主题模型）
	Restricted Boltzmann Machine（限制波兹曼机）

20. 高斯核
	这个核就是最开始提到过的会将原始空间映射为无穷维空间的那个家伙。不过，如果选得很大的话，高次特征上的权重实际上衰减得非常快，
	所以实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定
	是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调控参数，高斯核实际上具有相当高的灵活性，也是使用最广
	泛的核函数之一。

21. 经验风险、期望风险、结构风险
	经验风险是对训练集中的所有样本点损失函数的平均最小化。经验风险越小说明模型f(X)对训练集的拟合程度越好，但是对于未知的样本效果
	怎么样呢？我们知道未知的样本数据（<X,Y>）的数量是不容易确定的，所以就没有办法用所有样本损失函数的平均值的最小化这个方法，那么
	怎么来衡量这个模型对所有的样本（包含未知的样本和已知的训练样本）预测能力呢？熟悉概率论的很容易就想到了用期望。即假设X和Y服从
	联合分布P(X,Y).期望风险表示的是全局的概念，表示的是决策函数对所有的样本<X,Y>预测能力的大小，而经验风险则是局部的概念，仅仅表示
	决策函数对训练数据集里样本的预测能力。理想的模型（决策）函数应该是让所有的样本的损失函数最小的（也即期望风险最小化），但是期望
	风险函数往往是不可得到的，即上式中，X与Y的联合分布函数不容易得到。现在我们已经清楚了期望风险是全局的，理想情况下应该是让期望风
	险最小化，但是呢，期望风险函数又不是那么容易得到的。怎么办呢？那就用局部最优的代替全局最优这个思想吧。这就是经验风险最小化的理
	论基础。
	经验风险是局部的，基于训练集所有样本点损失函数最小化的。
	期望风险是全局的，是基于所有样本点的损失函数最小化的。
	经验风险函数是现实的，可求的；
	期望风险函数是理想化的，不可求的；
	结构风险是对经验风险和期望风险的折中。在经验风险函数后面加一个正则化项（惩罚项）便是结构风险了。
	相比于经验风险，结构风险多了一个惩罚项，其中是一个lamada是一个大于0的系数。J(f)表示的是是模型f的复杂度。结构风险可以这么理解：
	经验风险越小，模型决策函数越复杂，其包含的参数越多，当经验风险函数小到一定程度就出现了过拟合现象。也可以理解为模型决策函数的复
	杂程度是过拟合的必要条件，那么我们要想防止过拟合现象的方式，就要破坏这个必要条件，即降低决策函数的复杂度。也即，让惩罚项J(f)最
	小化，现在出现两个需要最小化的函数了。我们需要同时保证经验风险函数和模型决策函数的复杂度都达到最小化，一个简单的办法把两个式子
	融合成一个式子得到结构风险函数然后对这个结构风险函数进行最小化。

21. SVM的参数C和gamma
	SVM模型有两个非常重要的参数C与gamma。其中 C是惩罚系数，即对误差的宽容度。c越高，说明越不能容忍出现误差,容易过拟合。C越小，容易欠拟合。
	C过大或过小，泛化能力变差。
    gamma是选择RBF函数作为kernel后，该函数自带的一个参数。隐含地决定了数据映射到新的特征空间后的分布，gamma越大，支持向量越少，gamma值
	越小，支持向量越多。支持向量的个数影响训练与预测的速度。如果gamma设的太大，会很小，很小的高斯分布长得又高又瘦， 会造成只会作用于支持向
	量样本附近，对于未知样本分类效果很差，存在训练准确率可以很高，(如果让无穷小，则理论上，高斯核的SVM可以拟合任何非线性数据，但容易过拟合)
	而测试准确率不高的可能，就是通常说的过训练；而如果设的过小，则会造成平滑效应太大，无法在训练集上得到特别高的准确率，也会影响测试集的准确率。

22. 卷积层尺寸计算公式：
		输入图片大小 W×W
		Filter大小 F×F
		步长 S
		padding的像素数 P
		于是我们可以得出
		N = (W − F + 2P )/S+1
		输出图片大小为 N×N				当计算出现小数时，向下取整
	在tensorflow中的same和valid，如果 Padding='SAME'，输出尺寸为： W / S	如果 Padding='VALID'，输出尺寸为：(W - F + 1) / S
	也就是说，same尺度不变说的是步长为1的时候
	
23. pooling层尺度计算公式：
	输入：L*L*D
	池化参数: 大小size=F*F*d，滑动步长stride=S；
	输出：l*l*d
	则输出尺寸为：l=[（L-F）/S] + 1			当计算出现小数时，向上取整

24. 数据中的缺失值处理：
	常用的是取均值、众数等。树模型可以自然的处理数据中的缺失值，树可以训练无缺失值，后把缺失值按照权重分配给每个叶子节点。
	https://blog.csdn.net/u014204761/article/details/82838385		树模型为何能自然地处理数据中的缺失值
	https://blog.csdn.net/qq_19446965/article/details/81637199		数、RF、XGBOOST如何处理缺失值
	
25. xgboost特征选择：
	xgb.feature_importances_ 可以求出各个特征的重要性，内部计算方式是计算几个属性的得分。
	https://blog.csdn.net/sujinhehehe/article/details/84201415  weight、gain、cover简单举例计算
	https://blog.csdn.net/sunyaowu315/article/details/90664331   特征选择计算详解

26. 特征归一化：
	将特征缩放到给定的范围内。
	概率模型（树形模型）不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、RF。
	而像Adaboost、SVM、LR、Knn、KMeans之类的最优化问题就需要归一化。
	
27. 信息增益、信息增益比



































